  

---

  

**3. 准备工作**  

我们的目标是设计一套名为 $\pi$ 的控制策略，用于视觉导航。该策略以机器人当前和过去的 $RGB$ 观测作为输入，输出未来动作的分布。此外，策略可能还可以接收一个目标 $RGB$ 图像。  

  

当目标图像 $o_g$ 可用时，$\pi$ 需要采取有效的行动，最终到达目标位置。而在未知环境中，如果目标图像 $o_g$ 不可用，$\pi$ 则需要通过安全且合理的导航行为，探索环境并提供足够的行为覆盖。为了增强长期探索与目标定位能力，我们将策略 $\pi$ 与环境的拓扑记忆 $M$ 和高级规划器配对。这种结合能够鼓励机器人通过前往未探索区域的方式，有效探索环境。  

  

为了训练基于视觉输入的目标条件策略，我们使用了 $Visual Navigation Transformer(ViNT)$作为策略的主干。$ViNT$ 能够处理机器人当前的视觉观测 $o_t$ 和目标观测 $o_g$，并通过最大似然优化对地面真实动作和时间距离进行回归训练，从而实现目标条件导航。尽管 $ViNT$ 展示了出色的性能，但它无法支持无目标探索，且依赖外部的子目标生成机制。我们提出的 $NoMaD$ 方法扩展了 $ViNT$，使其既能处理目标条件导航，又能支持无目标探索任务。  

  

然而，仅依靠目标条件策略可能不足以应对需要长时间推理的大型环境导航。因此，我们将策略与情节性记忆 $M$ 结合。$M$ 是一个图结构，节点表示机器人在环境中的视觉观测，边表示由目标条件距离预测的可导航路径。在大型环境中，当单次视觉观测 $o_t$ 难以规划长时间轨迹时，机器人可以利用拓扑地图 $M$ 规划一系列子目标，从而逐步引导自己到达目标位置。  

  

---

  

**4. 方法**  

在本节中，我们介绍 $NoMaD$ 架构，这是一种目标条件的扩散策略，适用于目标达成和无目标探索任务。$NoMaD$ 包含两个核心组件：  

1. **基于注意力的目标遮罩**：提供灵活的机制，将策略条件化于（或屏蔽）可选的目标图像 $o_g$。  

2. **扩散策略**：提供了丰富的先验，用于生成机器人无碰撞动作。  

  

以下是对这两个组件的详细说明：  

  

**(A) 目标遮罩**  

为了训练能够同时执行目标达成与无目标探索的共享策略，我们对 $ViNT$ 架构进行了调整，增加了一个二值目标遮罩 $m$。该遮罩用于屏蔽目标标记，从而阻断策略的目标条件路径。  

  

当设置 $m=1$ 时，目标标记会被屏蔽，目标条件路径不会参与计算；当  $m=0$ 时，目标标记与观测标记一起用于策略计算。目标遮罩 $m$ 从伯努利分布中采样，采样概率为 $p_m$。在训练过程中，我们固定 $p_m = 0.5$，确保目标达成与无目标探索样本的数量相等。  

  

在测试时，我们根据实际需求设置 $m$ 的值：当需要无目标探索时，设置  $m=1$；当需要导航到指定目标图像时，设置$m=0$。实验表明，这种简单的遮罩机制非常有效，使单一策略能够胜任两种任务场景。  

  

---